{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBoO7_ZroDHh",
        "outputId": "aaf354ac-c89c-4e26-c66b-2f6ad159498a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nilearn\n",
            "  Downloading nilearn-0.10.1-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (4.9.2)\n",
            "Collecting nibabel>=3.2.0 (from nilearn)\n",
            "  Downloading nibabel-5.1.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.22.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (23.1)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.27.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.10.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->nilearn) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->nilearn) (3.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.1.5->nilearn) (1.16.0)\n",
            "Installing collected packages: nibabel, nilearn\n",
            "  Attempting uninstall: nibabel\n",
            "    Found existing installation: nibabel 3.0.2\n",
            "    Uninstalling nibabel-3.0.2:\n",
            "      Successfully uninstalled nibabel-3.0.2\n",
            "Successfully installed nibabel-5.1.0 nilearn-0.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nilearn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bkDR_YwhLTgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup environement"
      ],
      "metadata": {
        "id": "V88EP1XHLaMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QDvxAtYnVyH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import PIL\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import data\n",
        "from skimage.util import montage\n",
        "import skimage.transform as skTrans\n",
        "from skimage.transform import rotate\n",
        "from skimage.transform import resize\n",
        "from PIL import Image, ImageOps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nilearn.plotting as nlplt"
      ],
      "metadata": {
        "id": "gFA0fpWEBR6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pXySbwmKn-5V",
        "outputId": "158d40cf-efa6-47a3-b4fb-929e87839403"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/miykael/gif_your_nifti\n",
            "  Cloning https://github.com/miykael/gif_your_nifti to /tmp/pip-req-build-oflobiez\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/miykael/gif_your_nifti /tmp/pip-req-build-oflobiez\n",
            "  Resolved https://github.com/miykael/gif_your_nifti to commit bcf9c0a2f4a755792548085b2f766ad415f91385\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gif-your-nifti==0.2.0) (1.22.4)\n",
            "Requirement already satisfied: nibabel in /usr/local/lib/python3.10/dist-packages (from gif-your-nifti==0.2.0) (5.1.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from gif-your-nifti==0.2.0) (2.25.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gif-your-nifti==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio->gif-your-nifti==0.2.0) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (23.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gif-your-nifti==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->gif-your-nifti==0.2.0) (1.16.0)\n",
            "Building wheels for collected packages: gif-your-nifti\n",
            "  Building wheel for gif-your-nifti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gif-your-nifti: filename=gif_your_nifti-0.2.0-py3-none-any.whl size=6262 sha256=af8606ac8d77476fe7612f15d6468ad43e2d9e3d3f7ff175bec26fefb16ce601\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jmcwk_mk/wheels/3a/c2/0b/c08f2425925519bb014e107d2919dadc2556ec5e7c205e4472\n",
            "Successfully built gif-your-nifti\n",
            "Installing collected packages: gif-your-nifti\n",
            "Successfully installed gif-your-nifti-0.2.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3107\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3108\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3109\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2901\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2902\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: _DistInfoDistribution__dep_map",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3098\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3099\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3100\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m   2900\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2901\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2902\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: _pkg_info",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-210c87d3d3b2>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#import nilearn.plotting as nlplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgif_your_nifti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgif2nif\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gif_your_nifti/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gif_your_nifti\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \"\"\"\n\u001b[0;32m--> 966\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# push the new requirements onto the stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m             \u001b[0mnew_requirements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m             \u001b[0mrequirements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_requirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequires\u001b[0;34m(self, extras)\u001b[0m\n\u001b[1;32m   2819\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrequires\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextras\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m         \u001b[0;34m\"\"\"List of Requirements needed for this distro if `extras` are used\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2821\u001b[0;31m         \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2822\u001b[0m         \u001b[0mdeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mdeps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_dep_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3109\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dep_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_compute_dependencies\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0mreqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m         \u001b[0;31m# Including any condition expressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parsed_pkg_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Requires-Dist'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m             \u001b[0mreqs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_parsed_pkg_info\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3099\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3100\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3101\u001b[0;31m             \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPKG_INFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3102\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsestr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3103\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pkg_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_metadata\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1515\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_metadata_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1517\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_get\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1726\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1727\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/usr/local/lib/python3.10/dist-packages/nibabel-3.0.2.dist-info/METADATA'"
          ]
        }
      ],
      "source": [
        "\n",
        "# neural imaging\n",
        "import nilearn as nl\n",
        "import nibabel as nib\n",
        "#import nilearn.plotting as nlplt\n",
        "!pip install git+https://github.com/miykael/gif_your_nifti # nifti to gif\n",
        "import gif_your_nifti.core as gif2nif"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ml libs\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import CSVLogger\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "\n",
        "\n",
        "# Make numpy printouts easier to read.\n",
        "np.set_printoptions(precision=3, suppress=True)"
      ],
      "metadata": {
        "id": "Tszt8mmYLqB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définition des zones de segmentation et identification des 4 classes\n",
        "\n",
        "\n",
        "> Bloc en retrait\n",
        "\n"
      ],
      "metadata": {
        "id": "eNEqKILGStTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE seg-areas\n",
        "SEGMENT_CLASSES = {\n",
        "    0 : 'NOT tumor',\n",
        "    1 : 'NECROTIC/CORE', # or NON-ENHANCING tumor CORE\n",
        "    2 : 'EDEMA',\n",
        "    3 : 'ENHANCING' # original 4 -> converted into 3 later\n",
        "}\n",
        "\n",
        "# there are 100 slices per volume\n",
        "# to start at 22 and use 100 slices means we will skip the first 22 and last 22\n",
        "VOLUME_SLICES = 100\n",
        "VOLUME_START_AT = 22 # first slice of volume that we will include"
      ],
      "metadata": {
        "id": "MypG-Hn6MSaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "gWv1lCuCMVN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importation et visualisation des échantillons (imagesmédicale)du dataset Brain Tumour"
      ],
      "metadata": {
        "id": "Q0OALix_TJ3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flair=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_flair.nii.gz\")\n",
        "plt.imshow(flair.get_fdata()[:,50,:], cmap=plt.cm.bone)  # set the color map to bone\n",
        "plt.show()\n",
        "\n",
        "seg=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_seg.nii.gz\")\n",
        "plt.imshow(seg.get_fdata()[:,50,:], cmap=plt.cm.bone)  # set the color map to bone\n",
        "plt.show()\n",
        "\n",
        "\n",
        "t1=nib.load(\"//content/test/BraTS2021_00003/BraTS2021_00003_t1.nii.gz\")\n",
        "plt.imshow(t1.get_fdata()[:,50,:], cmap=plt.cm.bone)  # set the color map to bone\n",
        "plt.show()\n",
        "\n",
        "t1ce=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_t1ce.nii.gz\")\n",
        "\n",
        "plt.imshow(t1ce.get_fdata()[:,50,:], cmap=plt.cm.bone)  # set the color map to bone\n",
        "plt.show()\n",
        "\n",
        "t2=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_t2.nii.gz\")\n",
        "plt.imshow(t2.get_fdata()[:,50,:], cmap=plt.cm.bone)  # set the color map to bone\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SfISMatXMt7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importation des données, création et application du mask, essayer de segmenter et visualisation l'image importer"
      ],
      "metadata": {
        "id": "9AyAA2dWTzj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_DATASET_PATH = '/content/train/'\n",
        "VALIDATION_DATASET_PATH = '/content/validation'\n",
        "\n",
        "'''test_image_flair=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii').get_fdata()\n",
        "test_image_t1=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1.nii').get_fdata()\n",
        "test_image_t1ce=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t1ce.nii').get_fdata()\n",
        "test_image_t2=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_t2.nii').get_fdata()\n",
        "test_mask=nib.load(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_seg.nii').get_fdata()\n",
        "'''\n",
        "\n",
        "test_image_flair=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_flair.nii.gz\").get_fdata()\n",
        "test_image_t1=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_t1.nii.gz\").get_fdata()\n",
        "test_image_t1ce=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_t1ce.nii.gz\").get_fdata()\n",
        "test_image_t2=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_t2.nii.gz\").get_fdata()\n",
        "test_mask=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_seg.nii.gz\").get_fdata()\n",
        "\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1,5, figsize = (20, 10))\n",
        "slice_w = 25\n",
        "ax1.imshow(test_image_flair[:,:,test_image_flair.shape[0]//2-slice_w], cmap = 'gray')\n",
        "ax1.set_title('Image flair')\n",
        "ax2.imshow(test_image_t1[:,:,test_image_t1.shape[0]//2-slice_w], cmap = 'gray')\n",
        "ax2.set_title('Image t1')\n",
        "ax3.imshow(test_image_t1ce[:,:,test_image_t1ce.shape[0]//2-slice_w], cmap = 'gray')\n",
        "ax3.set_title('Image t1ce')\n",
        "ax4.imshow(test_image_t2[:,:,test_image_t2.shape[0]//2-slice_w], cmap = 'gray')\n",
        "ax4.set_title('Image t2')\n",
        "ax5.imshow(test_mask[:,:,test_mask.shape[0]//2-slice_w])\n",
        "ax5.set_title('Mask')"
      ],
      "metadata": {
        "id": "zqTZsZmYc39p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "On affiche des  n tranches de l image"
      ],
      "metadata": {
        "id": "6e71wbFq6RCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "z_LVgMS45sfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip 50:-50 slices since there is not much to see\n",
        "fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\n",
        "ax1.imshow(rotate(montage(test_image_t1[50:-50,:,:]), 90, resize=True), cmap ='gray')"
      ],
      "metadata": {
        "id": "40yjWRKrBoLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Application des tranfomations sur l'image mask niveau de gris (rotation, resize,  data augmentation)"
      ],
      "metadata": {
        "id": "w4GorQ4c6jxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Skip 50:-50 slices since there is not much to see\n",
        "fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\n",
        "ax1.imshow(rotate(montage(test_mask[60:-60,:,:]), 90, resize=True), cmap ='gray')"
      ],
      "metadata": {
        "id": "gAqjw8FcBpId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "creation d'une copie des résultats obtenues apres l'application des transformation 'Rotation'"
      ],
      "metadata": {
        "id": "WFyTNRPR63Z3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''shutil.copy2(TRAIN_DATASET_PATH + 'BraTS20_Training_001/BraTS20_Training_001_flair.nii', './test_gif_BraTS20_Training_001_flair.nii')\n",
        "gif2nif.write_gif_normal('./test_gif_BraTS20_Training_001_flair.nii')\n",
        "'''\n",
        "test_image_flair2=nib.load(\"/content/test/BraTS2021_00003/BraTS2021_00003_flair.nii.gz\").get_fdata()\n"
      ],
      "metadata": {
        "id": "3n5dp1YjBtD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot image.ni avec mask.ni anat,epi"
      ],
      "metadata": {
        "id": "l89bLmbJ7KpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "niimg = nl.image.load_img('/content/test/BraTS2021_00003/BraTS2021_00003_flair.nii.gz')\n",
        "nimask = nl.image.load_img('/content/test/BraTS2021_00003/BraTS2021_00003_seg.nii.gz')\n",
        "\n",
        "fig, axes = plt.subplots(nrows=4, figsize=(30, 40))\n",
        "\n",
        "\n",
        "nlplt.plot_anat(niimg,\n",
        "                title='BraTS20_Training_001_flair.nii plot_anat',\n",
        "                axes=axes[0])\n",
        "\n",
        "nlplt.plot_epi(niimg,\n",
        "               title='BraTS20_Training_001_flair.nii plot_epi',\n",
        "               axes=axes[1])\n",
        "\n",
        "nlplt.plot_img(niimg,\n",
        "               title='BraTS20_Training_001_flair.nii plot_img',\n",
        "               axes=axes[2])\n",
        "\n",
        "nlplt.plot_roi(nimask,\n",
        "               title='BraTS20_Training_001_flair.nii with mask plot_roi',\n",
        "               bg_img=niimg,\n",
        "               axes=axes[3], cmap='Paired')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u49jPQl2B4c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Définition d'une fonction de perte Dice coefficient, qui est essentiellement une mesure du chevauchement entre deux échantillons. Cette mesure varie de 0 à 1 où un coefficient de dés de 1 indique un chevauchement parfait et complet."
      ],
      "metadata": {
        "id": "wxlOcd9M8RxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dice loss as defined above for 4 classes\n",
        "def dice_coef(y_true, y_pred, smooth=1.0):\n",
        "    class_num = 4\n",
        "    for i in range(class_num):\n",
        "        y_true_f = K.flatten(y_true[:,:,:,i])\n",
        "        y_pred_f = K.flatten(y_pred[:,:,:,i])\n",
        "        intersection = K.sum(y_true_f * y_pred_f)\n",
        "        loss = ((2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth))\n",
        "   #     K.print_tensor(loss, message='loss value for class {} : '.format(SEGMENT_CLASSES[i]))\n",
        "        if i == 0:\n",
        "            total_loss = loss\n",
        "        else:\n",
        "            total_loss = total_loss + loss\n",
        "    total_loss = total_loss / class_num\n",
        "#    K.print_tensor(total_loss, message=' total dice coef: ')\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "\n",
        "# define per class evaluation of dice coef\n",
        "# inspired by https://github.com/keras-team/keras/issues/9395\n",
        "def dice_coef_necrotic(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,1] * y_pred[:,:,:,1]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,1])) + K.sum(K.square(y_pred[:,:,:,1])) + epsilon)\n",
        "\n",
        "def dice_coef_edema(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,2] * y_pred[:,:,:,2]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,2])) + K.sum(K.square(y_pred[:,:,:,2])) + epsilon)\n",
        "\n",
        "def dice_coef_enhancing(y_true, y_pred, epsilon=1e-6):\n",
        "    intersection = K.sum(K.abs(y_true[:,:,:,3] * y_pred[:,:,:,3]))\n",
        "    return (2. * intersection) / (K.sum(K.square(y_true[:,:,:,3])) + K.sum(K.square(y_pred[:,:,:,3])) + epsilon)\n",
        "\n",
        "\n",
        "\n",
        "# Computing Precision\n",
        "def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "\n",
        "# Computing Sensitivity\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (possible_positives + K.epsilon())\n",
        "\n",
        "\n",
        "# Computing Specificity\n",
        "def specificity(y_true, y_pred):\n",
        "    true_negatives = K.sum(K.round(K.clip((1-y_true) * (1-y_pred), 0, 1)))\n",
        "    possible_negatives = K.sum(K.round(K.clip(1-y_true, 0, 1)))\n",
        "    return true_negatives / (possible_negatives + K.epsilon())"
      ],
      "metadata": {
        "id": "yxhr4NPSCANy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# la taille de notre image d'entree\n",
        "IMG_SIZE=128"
      ],
      "metadata": {
        "id": "QD7Pg-s8k04t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#installation des bibliothéques nécessaires\n",
        "!pip install tensorflow==2.6.2"
      ],
      "metadata": {
        "id": "2gFBiWzmlXQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création de notre architecture UNet contient deux réseaux qui va travailler comme encoder decoder, où l'encoder va diminuer la dimension des entrées en utilisant des differents hyper-paramétres et des differnts fonctions afin d'optimiser les résultats\n",
        "\n",
        "la partie decodeur va essayer de recontriure la forme initiale du forme où on peut perdre des informations dans l'images initules."
      ],
      "metadata": {
        "id": "c5tHcSyu9o_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# source https://naomi-fridman.medium.com/multi-class-image-segmentation-a5cc671e647a\n",
        "#utilisée pour introduire la non-linéarité dans le CNN en utilisant et pour améliorer l’efficacité du traitement en intercalant entre les couches de traitement une couche qui va opérer une fonction mathématique la fonction ReLu ou la fonction sigmoïde\n",
        "\n",
        "\n",
        "def build_unet(inputs, ker_init, dropout):\n",
        "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(inputs)\n",
        "    conv1 = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv1)\n",
        "\n",
        "    pool = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool)\n",
        "    conv = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
        "\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv2)\n",
        "\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv3)\n",
        "\n",
        "\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(pool4)\n",
        "    conv5 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv5)\n",
        "    #Les couches dropout sont importantes dans l’entraînement des CNN car elles empêchent le surajustement des données d’entraînement.\n",
        "    drop5 = Dropout(dropout)(conv5)\n",
        "\n",
        "    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge7)\n",
        "    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv7)\n",
        "\n",
        "    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge8)\n",
        "    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv8)\n",
        "\n",
        "    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv,up9], axis = 3)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge9)\n",
        "    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv9)\n",
        "\n",
        "    up = Conv2D(32, 2, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(UpSampling2D(size = (2,2))(conv9))\n",
        "    merge = concatenate([conv1,up], axis = 3)\n",
        "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(merge)\n",
        "    conv = Conv2D(32, 3, activation = 'relu', padding = 'same', kernel_initializer = ker_init)(conv)\n",
        "\n",
        "    conv10 = Conv2D(4, (1,1), activation = 'softmax')(conv)\n",
        "\n",
        "    return Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "input_layer = Input((IMG_SIZE, IMG_SIZE, 2))\n",
        "\n",
        "model = build_unet(input_layer, 'he_normal', 0.2)\n",
        "#La fonction d’optimisation \" Adam\"\n",
        "#c’est une méthode de descente de gradient stochastique basée sur l’estimation\n",
        "#adaptative des moments du premier et du deuxième ordre.La descente de gra\u0002dient stochastique est une méthode itérative permettant d’optimiser une fonc\u0002tion objective avec des propriétés de lissage appropriée\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), metrics = ['accuracy',tf.keras.metrics.MeanIoU(num_classes=4), dice_coef, precision, sensitivity, specificity, dice_coef_necrotic, dice_coef_edema ,dice_coef_enhancing] )"
      ],
      "metadata": {
        "id": "Bvygp4aQCGJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model,\n",
        "           show_shapes = True,\n",
        "           show_dtype=False,\n",
        "           show_layer_names = True,\n",
        "           rankdir = 'TB',\n",
        "           expand_nested = False,\n",
        "           dpi = 70)"
      ],
      "metadata": {
        "id": "bmqlnyTjCK53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "nous avons travailler juste sur une partie de la  base de données utilisées en une partie train (6 images) test(2 images)  et validation (une image)"
      ],
      "metadata": {
        "id": "EzwEuKN4FNdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lists of directories with studies\n",
        "train_and_val_directories = [f.path for f in os.scandir('/content/train') if f.is_dir()]\n",
        "\n",
        "# file BraTS20_Training_355 has ill formatted name for for seg.nii file\n",
        "train_and_val_directories.remove('/content/train/BraTS2021_00005')\n",
        "\n",
        "\n",
        "def pathListIntoIds(dirList):\n",
        "    x = []\n",
        "    for i in range(0,len(dirList)):\n",
        "        x.append(dirList[i][dirList[i].rfind('/')+1:])\n",
        "    return x\n",
        "\n",
        "train_and_test_ids = pathListIntoIds(train_and_val_directories);\n",
        "\n",
        "\n",
        "train_test_ids, val_ids = train_test_split(train_and_test_ids,test_size=0.2)\n",
        "train_ids, test_ids = train_test_split(train_test_ids,test_size=0.15)"
      ],
      "metadata": {
        "id": "fyU3bnGHCQYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "id": "MOceTHfIrZtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.6.0"
      ],
      "metadata": {
        "id": "MPCrDVzHuCeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.all_utils.Sequence"
      ],
      "metadata": {
        "id": "AYFg6PExu6D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remplacer la classe DataGenerator de séquence Keras"
      ],
      "metadata": {
        "id": "gxda4w7RF5i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(keras.utils.data_utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, dim=(IMG_SIZE,IMG_SIZE), batch_size = 1, n_channels = 2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.list_IDs = list_IDs\n",
        "        self.n_channels = n_channels\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        Batch_ids = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(Batch_ids)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, Batch_ids):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
        "        y = np.zeros((self.batch_size*VOLUME_SLICES, 240, 240))\n",
        "        Y = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, 4))\n",
        "\n",
        "\n",
        "        # Generate data\n",
        "        for c, i in enumerate(Batch_ids):\n",
        "            case_path = os.path.join(/content/train, i)\n",
        "\n",
        "            data_path = os.path.join(case_path, f'{i}_flair.nii');\n",
        "            flair = nib.load(data_path).get_fdata()\n",
        "\n",
        "            data_path = os.path.join(case_path, f'{i}_t1ce.nii');\n",
        "            ce = nib.load(data_path).get_fdata()\n",
        "\n",
        "            data_path = os.path.join(case_path, f'{i}_seg.nii');\n",
        "            seg = nib.load(data_path).get_fdata()\n",
        "\n",
        "            for j in range(VOLUME_SLICES):\n",
        "                 X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
        "                 X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
        "\n",
        "                 y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n",
        "\n",
        "        # Generate masks\n",
        "        y[y==4] = 3;\n",
        "        mask = tf.one_hot(y, 4);\n",
        "        Y = tf.image.resize(mask, (IMG_SIZE, IMG_SIZE));\n",
        "        return X/np.max(X), Y\n",
        "\n",
        "training_generator = DataGenerator(train_ids)\n",
        "valid_generator = DataGenerator(val_ids)\n",
        "test_generator = DataGenerator(test_ids)"
      ],
      "metadata": {
        "id": "d1WBo4Z7CWMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of data used for training / testing / validation"
      ],
      "metadata": {
        "id": "KoAIgV3vGViL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# show number of data for each dir\n",
        "def showDataLayout():\n",
        "    plt.bar([\"Train\",\"Valid\",\"Test\"],\n",
        "    [len(train_ids), len(val_ids), len(test_ids)], align='center',color=[ 'green','red', 'blue'])\n",
        "    plt.legend()\n",
        "\n",
        "    plt.ylabel('Number of images')\n",
        "    plt.title('Data distribution')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "showDataLayout()"
      ],
      "metadata": {
        "id": "quL3PuUsCcGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model\n",
        "My best model was trained with 81% accuracy on mean IOU and 65.5% on Dice loss\n",
        "I will load this pretrained model instead of training again"
      ],
      "metadata": {
        "id": "e1Kmzvc0Gb6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_logger = CSVLogger('training.log', separator=',', append=False)\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "#     keras.callbacks.EarlyStopping(monitor='loss', min_delta=0,\n",
        "#                               patience=2, verbose=1, mode='auto'),\n",
        "      keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n",
        "                              patience=2, min_lr=0.000001, verbose=1),\n",
        "#  keras.callbacks.ModelCheckpoint(filepath = 'model_.{epoch:02d}-{val_loss:.6f}.m5',\n",
        "#                             verbose=1, save_best_only=True, save_weights_only = True)\n",
        "        csv_logger\n",
        "    ]"
      ],
      "metadata": {
        "id": "uxjqakt4CgR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K.clear_session()\n",
        "\n",
        "# history =  model.fit(training_generator,\n",
        "#                     epochs=35,\n",
        "#                     steps_per_epoch=len(train_ids),\n",
        "#                     callbacks= callbacks,\n",
        "#                     validation_data = valid_generator\n",
        "#                     )\n",
        "# model.save(\"model_x1_1.h5\")"
      ],
      "metadata": {
        "id": "PDlLb6wECjCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualiser le processus de \"training \""
      ],
      "metadata": {
        "id": "NHaGFa1jaxZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "############ load trained model ################\n",
        "model = keras.models.load_model('/content/model-per-class-eval/model_per_class.h5',\n",
        "                                   custom_objects={ 'accuracy' : tf.keras.metrics.MeanIoU(num_classes=4),\n",
        "                                                   \"dice_coef\": dice_coef,\n",
        "                                                   \"precision\": precision,\n",
        "                                                   \"sensitivity\":sensitivity,\n",
        "                                                   \"specificity\":specificity,\n",
        "                                                   \"dice_coef_necrotic\": dice_coef_necrotic,\n",
        "                                                   \"dice_coef_edema\": dice_coef_edema,\n",
        "                                                   \"dice_coef_enhancing\": dice_coef_enhancing\n",
        "                                                  }, compile=False)\n",
        "\n",
        "history = pd.read_csv('/content/model-per-class-eval/training_per_class.log', sep=',', engine='python')\n",
        "\n",
        "hist=history\n",
        "\n",
        "############### ########## ####### #######\n",
        "\n",
        "# hist=history.history\n",
        "\n",
        "acc=hist['accuracy']\n",
        "val_acc=hist['val_accuracy']\n",
        "\n",
        "epoch=range(len(acc))\n",
        "\n",
        "loss=hist['loss']\n",
        "val_loss=hist['val_loss']\n",
        "\n",
        "train_dice=hist['dice_coef']\n",
        "val_dice=hist['val_dice_coef']\n",
        "\n",
        "f,ax=plt.subplots(1,4,figsize=(16,8))\n",
        "\n",
        "ax[0].plot(epoch,acc,'b',label='Training Accuracy')\n",
        "ax[0].plot(epoch,val_acc,'r',label='Validation Accuracy')\n",
        "ax[0].legend()\n",
        "\n",
        "ax[1].plot(epoch,loss,'b',label='Training Loss')\n",
        "ax[1].plot(epoch,val_loss,'r',label='Validation Loss')\n",
        "ax[1].legend()\n",
        "\n",
        "ax[2].plot(epoch,train_dice,'b',label='Training dice coef')\n",
        "ax[2].plot(epoch,val_dice,'r',label='Validation dice coef')\n",
        "ax[2].legend()\n",
        "\n",
        "ax[3].plot(epoch,hist['mean_io_u'],'b',label='Training mean IOU')\n",
        "ax[3].plot(epoch,hist['val_mean_io_u'],'r',label='Validation mean IOU')\n",
        "ax[3].legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "52axWSimCnI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mri type must one of 1) flair 2) t1 3) t1ce 4) t2 ------- or even 5) seg\n",
        "# returns volume of specified study at `path`\n",
        "def imageLoader(path):\n",
        "    image = nib.load(path).get_fdata()\n",
        "    X = np.zeros((self.batch_size*VOLUME_SLICES, *self.dim, self.n_channels))\n",
        "    for j in range(VOLUME_SLICES):\n",
        "        X[j +VOLUME_SLICES*c,:,:,0] = cv2.resize(image[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
        "        X[j +VOLUME_SLICES*c,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE));\n",
        "\n",
        "        y[j +VOLUME_SLICES*c] = seg[:,:,j+VOLUME_START_AT];\n",
        "    return np.array(image)\n",
        "\n",
        "\n",
        "# load nifti file at `path`\n",
        "# and load each slice with mask from volume\n",
        "# choose the mri type & resize to `IMG_SIZE`\n",
        "def loadDataFromDir(path, list_of_files, mriType, n_images):\n",
        "    scans = []\n",
        "    masks = []\n",
        "    for i in list_of_files[:n_images]:\n",
        "        fullPath = glob.glob( i + '/*'+ mriType +'*')[0]\n",
        "        currentScanVolume = imageLoader(fullPath)\n",
        "        currentMaskVolume = imageLoader( glob.glob( i + '/*seg*')[0] )\n",
        "        # for each slice in 3D volume, find also it's mask\n",
        "        for j in range(0, currentScanVolume.shape[2]):\n",
        "            scan_img = cv2.resize(currentScanVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n",
        "            mask_img = cv2.resize(currentMaskVolume[:,:,j], dsize=(IMG_SIZE,IMG_SIZE), interpolation=cv2.INTER_AREA).astype('uint8')\n",
        "            scans.append(scan_img[..., np.newaxis])\n",
        "            masks.append(mask_img[..., np.newaxis])\n",
        "    return np.array(scans, dtype='float32'), np.array(masks, dtype='float32')\n",
        "\n",
        "#brains_list_test, masks_list_test = loadDataFromDir(VALIDATION_DATASET_PATH, test_directories, \"flair\", 5)"
      ],
      "metadata": {
        "id": "7mb46JN7C8uI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** *texte en italique*BraTS2021 prediction**\n",
        "\n",
        "1.  BraTS2021_00009\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "k3Yn8t_i0It6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictByPath(case_path,case):\n",
        "    files = next(os.walk(case_path))[2]\n",
        "    X = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE, 2))\n",
        "  #  y = np.empty((VOLUME_SLICES, IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    vol_path = os.path.join(case_path, f'/content/train/BraTS2021_00009/BraTS2021_{case}_flair.nii.gz');\n",
        "    flair=nib.load(vol_path).get_fdata()\n",
        "\n",
        "    vol_path = os.path.join(case_path, f'/content/train/BraTS2021_00009/BraTS2021_{case}_t1ce.nii.gz');\n",
        "    ce=nib.load(vol_path).get_fdata()\n",
        "\n",
        " #   vol_path = os.path.join(case_path, f'BraTS20_Training_{case}_seg.nii');\n",
        " #   seg=nib.load(vol_path).get_fdata()\n",
        "\n",
        "\n",
        "    for j in range(VOLUME_SLICES):\n",
        "        X[j,:,:,0] = cv2.resize(flair[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        "        X[j,:,:,1] = cv2.resize(ce[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        " #       y[j,:,:] = cv2.resize(seg[:,:,j+VOLUME_START_AT], (IMG_SIZE,IMG_SIZE))\n",
        "\n",
        "  #  model.evaluate(x=X,y=y[:,:,:,0], callbacks= callbacks)\n",
        "    return model.predict(X/np.max(X), verbose=1)\n",
        "\n",
        "\n",
        "def showPredictsById(case, start_slice = 60):\n",
        "    path = f\"/content/train/BraTS2021_{case}\"\n",
        "    gt = nib.load(os.path.join(path, f'/content/train/BraTS2021_00009/BraTS2021_{case}_seg.nii.gz')).get_fdata()\n",
        "\n",
        "\n",
        "    origImage = nib.load(os.path.join(path, f'/content/train/BraTS2021_00009/BraTS2021_{case}_flair.nii.gz')).get_fdata()\n",
        "    p = predictByPath(path,case)\n",
        "\n",
        "    core = p[:,:,:,1]\n",
        "    edema= p[:,:,:,2]\n",
        "    enhancing = p[:,:,:,3]\n",
        "\n",
        "    plt.figure(figsize=(18, 50))\n",
        "    f, axarr = plt.subplots(1,6, figsize = (18, 50))\n",
        "\n",
        "    for i in range(6): # for each image, add brain background\n",
        "        axarr[i].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\", interpolation='none')\n",
        "\n",
        "    axarr[0].imshow(cv2.resize(origImage[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE)), cmap=\"gray\")\n",
        "    axarr[0].title.set_text('Original image flair')\n",
        "    curr_gt=cv2.resize(gt[:,:,start_slice+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE), interpolation = cv2.INTER_NEAREST)\n",
        "    axarr[1].imshow(curr_gt, cmap=\"Reds\", interpolation='none', alpha=0.3) # ,alpha=0.3,cmap='Reds'\n",
        "    axarr[1].title.set_text('Ground truth')\n",
        "    axarr[2].imshow(p[start_slice,:,:,1:4], cmap=\"Reds\", interpolation='none', alpha=0.3)\n",
        "    axarr[2].title.set_text('all classes')\n",
        "    axarr[3].imshow(edema[start_slice,:,:], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
        "    axarr[3].title.set_text(f'{SEGMENT_CLASSES[1]} predicted')\n",
        "    axarr[4].imshow(core[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
        "    axarr[4].title.set_text(f'{SEGMENT_CLASSES[2]} predicted')\n",
        "    axarr[5].imshow(enhancing[start_slice,:,], cmap=\"OrRd\", interpolation='none', alpha=0.3)\n",
        "    axarr[5].title.set_text(f'{SEGMENT_CLASSES[3]} predicted')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "showPredictsById(case=test_ids[0][-5:])\n",
        "'''showPredictsById(case=test_ids[1][-5:])\n",
        "showPredictsById(case=test_ids[2][-5:])\n",
        "showPredictsById(case=test_ids[3][-5:])\n",
        "showPredictsById(case=test_ids[4][-5:])\n",
        "showPredictsById(case=test_ids[5][-5:])\n",
        "showPredictsById(case=test_ids[6][-5:])'''\n",
        "\n",
        "\n",
        "# mask = np.zeros((10,10))\n",
        "# mask[3:-3, 3:-3] = 1 # white square in black background\n",
        "# im = mask + np.random.randn(10,10) * 0.01 # random image\n",
        "# masked = np.ma.masked_where(mask == 0, mask)\n",
        "\n",
        "# plt.figure()\n",
        "# plt.subplot(1,2,1)\n",
        "# plt.imshow(im, 'gray', interpolation='none')\n",
        "# plt.subplot(1,2,2)\n",
        "# plt.imshow(im, 'gray', interpolation='none')\n",
        "# plt.imshow(masked, 'jet', interpolation='none', alpha=0.7)# plt.show()"
      ],
      "metadata": {
        "id": "0VBQLCQaDCcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case = case=test_ids[0][-5:]\n",
        "path = f\"/content/train/BraTS2021_{case}\"\n",
        "gt = nib.load(os.path.join(path, f'/content/train/BraTS2021_00009/BraTS2021_{case}_seg.nii.gz')).get_fdata()\n",
        "p = predictByPath(path,case)\n",
        "\n",
        "\n",
        "core = p[:,:,:,1]\n",
        "edema= p[:,:,:,2]\n",
        "enhancing = p[:,:,:,3]\n",
        "\n",
        "\n",
        "i=40 # slice at\n",
        "eval_class = 2 #     0 : 'NOT tumor',  1 : 'ENHANCING',    2 : 'CORE',    3 : 'WHOLE'\n",
        "\n",
        "\n",
        "\n",
        "gt[gt != eval_class] = 1 # use only one class for per class evaluation\n",
        "\n",
        "resized_gt = cv2.resize(gt[:,:,i+VOLUME_START_AT], (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "plt.figure()\n",
        "f, axarr = plt.subplots(1,2)\n",
        "axarr[0].imshow(resized_gt, cmap=\"gray\")\n",
        "axarr[0].title.set_text('ground truth')\n",
        "axarr[1].imshow(p[i,:,:,eval_class], cmap=\"gray\")\n",
        "axarr[1].title.set_text(f'predicted class: {SEGMENT_CLASSES[eval_class]}')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwRqhRtwDIWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHehw99d0AmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "lQed-wQq0zSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nouvelle section"
      ],
      "metadata": {
        "id": "Puk6A0w10z63"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}